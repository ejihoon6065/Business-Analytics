{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝 방법\n",
    "### NLP(Natural Language Processing) 기본도구\n",
    "    - Tokenize, stemming, lemmatize\n",
    "    - Chunking\n",
    "    - BOW, TFIDF – sparse representation\n",
    "### 머신러닝(딥러닝)\n",
    "    - Naïve Bayes, Logistic egression, Decision tree, SVM\n",
    "    - Embedding(Word2Vec, Doc2Vec) – dense representation\n",
    "    - RNN(LSTM), Attention, Transformer\n",
    "    \n",
    "## 텍스트 마이닝 적용분야\n",
    "1. Document classification\n",
    "2. Doucument generation\n",
    "3. Keyword extraction\n",
    "4. Topic modeling\n",
    "\n",
    "## 텍스트 마이닝 기본도구\n",
    "- 목적: document, sentence 등을 sparse vector로 변환\n",
    "- Tokenize\n",
    "    - 대상이 되는 문서/문장을 최소 단위로 쪼갬\n",
    "- Text normalization\n",
    "    - 최소 단위를 표준화\n",
    "- POS-tagging\n",
    "    - 최소 의미단위로 나누어진 대상에 대해 품사를 부착\n",
    "- Chunking\n",
    "    - POS-tagging의 결과를 명사구, 형용사구, 분사구 등과 같은 말모듬으로 다시 합치는 과정\n",
    "- BOW, TFIDF\n",
    "    - tokenized 결과를 이용하여 문서를 vector로 표현\n",
    "    \n",
    "### Tokenize\n",
    "- Tokenize\n",
    "    - Document를 Sentence의 집합으로 분리\n",
    "    - Sentence를 Word의 집합으로 분리\n",
    "    - 의미 없는 문자 등을 걸러 냄\n",
    "- 영어 vs. 한글\n",
    "    - 영어는 공백(space) 기준으로 비교적 쉽게 tokenize 가능\n",
    "    - 한글은 구조상 형태소(morpheme) 분석이 필요\n",
    "        - 복합명사, 조사, 어미 등을 분리해내는 작업이 필요\n",
    "        - 영어에 비해 어렵고 정확도 낮음\n",
    "        \n",
    "### Text Normalization\n",
    "- 동일한 의미의 단어가 다른 형태를 갖는 것을 보완\n",
    "    - 다른 형태의 단어들을 통일시켜 표준단어로 변환\n",
    "- Stemming (어간 추출)\n",
    "    - 단수 – 복수, 현재형 – 미래형 등 단어의 다양한 변형을 하나로 통일\n",
    "    - 의미가 아닌 규칙에 의한 변환\n",
    "    - 영어의 경우, Porter stemmer, Lancaster stemmer 등이 유명\n",
    "- Lemmatization (표제어 추출)\n",
    "    - 사전을 이용하여 단어의 원형을 추출\n",
    "    - 품사(part-of-speech)를 고려\n",
    "    - 영어의 경우, 유명한 어휘 데이터베이스인 WordNet을 이용한 WordNet lemmatizer가 많이 쓰임\n",
    "    \n",
    "### POS-tagging\n",
    "- 토큰화와 정규화 작업을 통해 나누어진 형태소(의미를가지는 최소단위)에 대해 품사를 결정하여 할당하는 작업\n",
    "- 동일한 단어라도 문맥에 따라 의미가 달라지므로 품사를 알기 위해서는 문맥을 파악해야 함\n",
    "- Text-to-speech(텍스트를 읽어주는 시스템)에서도 각 단어에 대해 올바른 발음을 하기 위해 품사 태깅을 이용\n",
    "- POS Tagging은 형태소 분석으로 번역되기도 하는데, 형태소 분석은 주어진 텍스트(원시말뭉치)를 형태소 단위로 나누는 작업을 포함하므로 앞의 토큰화, 정규화 작업에 품사 태깅을 포함한 것으로 보는 것이 타당\n",
    "\n",
    "### Chunking\n",
    "- Chunk는 언어학적으로 말모듬을 뜻하며, 명사구, 형용사구, 분사구 등과 같이 주어와 동사가 없는 두 단어 이상의 집합인 구(phrase)를 의미\n",
    "- Chunking은 주어진 텍스트에서 이와 같은 chunk를 찾는 과정\n",
    "- 즉, 형태소 분석의 결과인 각 형태소들을 서로 겹치지 않으면서 의미가 있는 구로 묶어나가는 과정임\n",
    "- 텍스트로부터 Information Extraction(정보추출)을 하기 위한 전단계로 보거나 혹은 Information Extraction에 포함되기도 함\n",
    "\n",
    "### BOW\n",
    "- Vector space model\n",
    "- count vector\n",
    "\n",
    "### TFIDF\n",
    "- count vector의 문제점\n",
    "    - 많은 문서에 공통적으로 나타난 단어는 중요성이 떨어지는 단어일 가능성이 높음 \n",
    "- TFIDF\n",
    "    - 단어의 count를 단어가 나타난 문서의 수로 나눠서 자주등장하지 않는 단어의 weight를 올림\n",
    "    \n",
    "### Text classification with BOW/TFIDF\n",
    "- Naive Bayes\n",
    "    - text categorization에서 인기 있는 방법\n",
    "- Logistic Regression\n",
    "    - 분류를 위한 회구분석\n",
    "    - 텍스트 마이닝에서의 문제\n",
    "- Ridge and Lasso Regression\n",
    "    - 릿지 회귀\n",
    "    - 라쏘 회귀\n",
    "    \n",
    "### 텍스트 마이닝의 문제점\n",
    "1. Curse of Dimensionality\n",
    "    - 차원의 저주\n",
    "        - extremely sparse data\n",
    "        - 각 데이터 간의 거리가 너무 멀게 위치\n",
    "    - 해결방법\n",
    "        - 더 많은 데이터\n",
    "2. 단어 빈도의 불균형\n",
    "    - 멱법칩\n",
    "        - 극히 소수의 데이터가 정적인 영향을 미치게 됨\n",
    "3. 단어가 쓰인 순서정보의 손실\n",
    "    - 통계에 의한 의미 파악 vs. 순서에 의한 의미 파악\n",
    "    - Loss of sequence information\n",
    "\n",
    "### 해결방안\n",
    "1. 차원축소\n",
    "    - feature selection\n",
    "    - feature extraction\n",
    "    - embedding\n",
    "    - deep learning\n",
    "\n",
    "### Topic Modeling\n",
    "- Documents are mixtures of topics\n",
    "- A topic is a probability distribution over words\n",
    "- A topic model is a generative model for documents\n",
    "    - Different documents can be produced by picking words from a topic depending on the weight given to the topic\n",
    "- Infer the set of topics that were responsible for generating a collection of documents\n",
    "- 활용사례: 드라마 시청률 변화 <-> 소설미디어 토픽 변화\n",
    "\n",
    "### Word Embedding\n",
    "- one-hot-encoding으로 표현된 단어를 dense vector로 변환\n",
    "- 변환된 vector를 이용하여 학습\n",
    "- 최종목적에 맞게 학습에 의해 vector가 결정됨\n",
    "- 학습목적 관점에서의 단어의 의미를 내포\n",
    "\n",
    "### one hot encoding\n",
    "- 각 단어를 모든 문서에서 사용된 단어들의 수 길이의 벡터로 표현, 심한 경우 길이 30만의 벡터 중에서 하나만 1인 sparse vector가 됨\n",
    "\n",
    "### Bow 와 word embedding의 차이점\n",
    "- bow는 raw text를 fixed length vector로 바꾸는 것\n",
    "- embedding은 단어를 one-hot-vector로 바꾸고 embedded vector로 바꿈\n",
    "\n",
    "### word embedding을 이용한 문서 분류\n",
    "- BOW와는 다른 관점의 문서 표현\n",
    "    - document: 제한된 maxlen 개의 word sequence (앞이나 뒤를 잘라냄)\n",
    "    - word: one-hot-vector에서 저차원(reduced_dim)으로 embedding된 dense vector\n",
    "    - 즉 document는 (maxlen, reduced_dim)의 2차원 행렬로 표현\n",
    "- 단순한 분류모형 (sequence 무시)\n",
    "    - (maxlen, reduced_dim) 차원의 document를maxlen*reduced_dim 차원으로 펼쳐서 분류모형에 적용\n",
    "    \n",
    "### Word2Vec\n",
    "- 문장에 나타난 단어들의 순서를 이용해 word embedding을 수행\n",
    "    - CBOW: 주변단어들을 이용해 다음 단어를 예측\n",
    "    - Skip-gram: 한 단어의 주변단어들을 예측\n",
    "- 단어의 위치에 기반하여 의미를 내포하는 vector 생성\n",
    "    - 비슷한 위치에 나타나는 단어들은 비슷한 vector를 가지게됨\n",
    "    - 단어 간의 유사성을 이용하여 연산이 가능\n",
    "    \n",
    "### ELMo(Embeddings from Language Model)\n",
    "- 사전 훈련된 언어 모델을 사용하는 워드 임베딩 방법론. \n",
    "- 이전의 대표적인 임베딩 기법인 Word2Vec이나 GloVe 등이 동일한 단어가 문맥에 따라 전혀 다른 의미를 가지는 것을 반영하지 못하는 것에 비해, ELMo는 이러한 문맥을 반영하기 위해 개발된 워드 임베딩 기법. \n",
    "- 문맥의 파악을 위해 biLSTM으로 학습된 모형을 이용\n",
    "\n",
    "### Document Embedding\n",
    "- Word2Vec은 word에 대해 dense vector를 생성하지만,document vector는 여전히 sparse\n",
    "- Word2Vec 모형에서 주변단어들에 더하여 document의 고유한 vector를 함께 학습함으로써 document에 대한 dense vector를 생성\n",
    "- 이 dense vector를 이용해 매칭, 분류 등의 작업 수행\n",
    "\n",
    "### 차원의 저주를 해결하는 방법\n",
    "1. RMB\n",
    "    - G.Hinton에 의해 제안\n",
    "    - 차원을 변경하면서 원래의 정보량 유지가 목적\n",
    "2. Autoencoder\n",
    "    - RBM과 유사한 개념\n",
    "        - encoder로 차원을 축소하고 decoder로 다시 복원했을 때, 원래의 X와 복원한 X’이 최대한 동일하도록 학습\n",
    "    - 작동방식은 PCA와 유사\n",
    "        - 데이터에 내재된 일정한구조 – 연관성을 추출\n",
    "3. Context의 파악\n",
    "    - N-gram\n",
    "        - 문맥(context)를 파악하기 위한 전통적 방법\n",
    "        - bi-gram, tri-gram, …\n",
    "        - 대상이 되는 문자열을 하나의 단어 단위가 아닌, 두개 이상의 단위로 잘라서 처리\n",
    "    - 딥러닝 – RNN\n",
    "        - 문장을 단어들의 sequence 혹은 series로 처리\n",
    "        - 뒷 단어에 대한 hidden node가 앞 단어의 hidden node 값에도 영향을 받도록 함\n",
    "        - 그 외에도 단어들 간의 관계를 학습할 수 있는 모형을 고안\n",
    "4. LSTM\n",
    "    - RNN의 문제\n",
    "        - 문장이 길수록 층이 깊은 형태를 갖게 됨 -> 경사가 소실되는 문제 발생 -> 앞부분의 단어 정보가 학습되지 않음\n",
    "        - LSTM: 직통 통로를 만들어 RNN의 문제를 해결\n",
    "\n",
    "5. CNN\n",
    "    - CNN은 원래 이미지 처리를 위해 개발된 신경망으로, 현재는 인간의 이미지 인식보다 더 나은 인식 성능을 보이고 있음.\n",
    "\n",
    "6. Sequence-to-sequence\n",
    "    - 지금까지는 입력은 sequence, 출력은 하나의 값인 경우가 일반적이었으나, 번역, chat-bot, summarize등은 출력도 sequence가 되어야 함\n",
    "    - encoder, decoder의 구조를 가짐\n",
    "    \n",
    "7. Attention\n",
    "    - 출력에 나온 어떤 단어는 입력에 있는 특정 단어들에 민감한 것에 착안\n",
    "    - 입력의 단어들로부터 출력 단어에 직접 링크를 만듦\n",
    "\n",
    "8. Transformer\n",
    "    - 입력 단어들끼리도 상호 연관성이 있는 것에 착안\n",
    "    - encoder와 decoder가 서로 다른 attention 구조를 사용\n",
    "\n",
    "9. BERT\n",
    "    - 양방향 transfomer 인코더를 사용\n",
    "    - transfer learning에서 feature + model을 함께 transfer하고 fine tuning을 통해서 적용하는 방식을 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 웹 크롤링1 - Static Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다\n",
      "저장되었습니다..\n"
     ]
    }
   ],
   "source": [
    "from urllib import request #웹 사이트에 있는 데이터를 추출하기 위해 ullib 라이브러리 사용\n",
    "\n",
    "url=\"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename=\"test.png\"\n",
    "\n",
    "request.urlretrieve(url, savename) #urllib.request 모듈에 있는 urlretrieve() 함수 이용\n",
    "print(\"저장되었습니다\")\n",
    "\n",
    "# URL과 저장경로 지정하기\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"test1.png\"\n",
    "#다운로드\n",
    "mem = request.urlopen(url).read()\n",
    "#파일로 저장하기, wb는 쓰기와 바이너리모드\n",
    "with open(savename, mode=\"wb\") as f:\n",
    "    f.write(mem)\n",
    "    print(\"저장되었습니다..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클라이언트 접속 정보 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ip]\n",
      "API_URI=http://api.aoikujira.com/ip/get.php\n",
      "REMOTE_ADDR=121.167.35.101\n",
      "REMOTE_HOST=121.167.35.101\n",
      "REMOTE_PORT=48124\n",
      "HTTP_HOST=api.aoikujira.com\n",
      "HTTP_USER_AGENT=Python-urllib/3.8\n",
      "HTTP_ACCEPT_LANGUAGE=\n",
      "HTTP_ACCEPT_CHARSET=\n",
      "SERVER_PORT=80\n",
      "FORMAT=ini\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#데이터 읽어들이기\n",
    "url=\"http://api.aoikujira.com/ip/ini\"\n",
    "res=request.urlopen(url)\n",
    "data=res.read()\n",
    "\n",
    "#바이너리를 문자열로 변환하기\n",
    "text=data.decode(\"utf-8\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BeautifulSoup\n",
    "- 파이썬으로 스크레이핑할 때 사용되는 라이브러리로서 HTML/XML에서 정보를 추출할 수 있도록 도와줌. 그러나 다운로드 기능은 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "  <h1>스크레이핑이란?</h1>\n",
    "  <p>웹 페이지를 분석하는 것</p>\n",
    "  <p>원하는 부분을 추출하는 것</p>\n",
    "</body></html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 스크레이핑이란?\n",
      "p  = 웹 페이지를 분석하는 것\n",
      "p  = 원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser') # HTML 분석\n",
    "\n",
    "#원하는 부분 추출\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "p2 = p1.next_sibling.next_sibling\n",
    "\n",
    "#요소의 글자 출력\n",
    "print(f\"h1 = {h1.string}\")\n",
    "print(f\"p  = {p1.string}\")\n",
    "print(f\"p  = {p2.string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>스크레이핑이란?</h1>\n",
      "#title = 스크레이핑이란?\n",
      "#body = 웹 페이지를 분석하는 것\n"
     ]
    }
   ],
   "source": [
    "# 단일 요소 추출: find()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "title = soup.find(\"h1\")\n",
    "body  = soup.find(\"p\")\n",
    "print(title)\n",
    "\n",
    "#텍스트 부분 출력하기\n",
    "print(f\"#title = {title.string}\" )\n",
    "print(f\"#body = {body.string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"http://www.naver.com\">naver</a>, <a href=\"http://www.daum.net\">daum</a>] 2\n",
      "naver > http://www.naver.com\n",
      "daum > http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "#복수 요소 추출: find_all()\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "  <ul>\n",
    "    <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "    <li><a href=\"http://www.daum.net\">daum</a></li>\n",
    "  </ul>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "links = soup.find_all(\"a\")\n",
    "print(links, len(links))\n",
    "\n",
    "#링크 목록 출력\n",
    "for a in links:\n",
    "    href = a.attrs['href']\n",
    "    text = a.string \n",
    "    print(text, \">\", href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Css Selector\n",
    "- 웹상의 요소에 css를 적용하기 위한 문법으로, 즉 요소를 선택하기 위한 패턴입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 위키북스 도서\n",
      "li = 유니티 게임 이펙트 입문\n",
      "li = 스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "li = 모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "html = \"\"\"\n",
    "<html><body>\n",
    "<div id=\"meigen\">\n",
    "  <h1>위키북스 도서</h1>\n",
    "  <ul class=\"items\">\n",
    "    <li>유니티 게임 이펙트 입문</li>\n",
    "    <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
    "    <li>모던 웹사이트 디자인의 정석</li>\n",
    "  </ul>\n",
    "</div>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#필요한 부분을 css쿼리로 추출\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string\n",
    "print(f\"h1 = {h1}\")\n",
    "li_list = soup.select(\"div#meigen > ul.items > li\")\n",
    "for li in li_list:\n",
    "  print(f\"li = {li.string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 활용예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request, parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. 네이버 금융-환율 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd/krw = 1,175.00\n"
     ]
    }
   ],
   "source": [
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = request.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "price = soup.select_one(\"div.head_info > span.value\").string\n",
    "print(\"usd/krw =\", price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. 기상청 RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url= http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\n",
      "서울,경기도 육상중기예보\n",
      "○ (강수) 10월 1일(목) 오후~2일(금) 오전에는 비가 내리겠습니다.<br />○ (기온) 이번 예보기간 낮 기온은 19~25도로 어제(26일, 24~26도)보다 낮겠고, 아침 기온은 6~17도로 선선하겠습니다.<br />          특히, 내륙을 중심으로 낮과 밤의 기온차가 10도 내외로 크겠습니다.<br />○ (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다.\n",
      "서울,경기도 육상중기예보\n",
      "○ (강수) 10월 1일(목) 오후~2일(금) 오전에는 비가 내리겠습니다.<br />○ (기온) 이번 예보기간 낮 기온은 19~25도로 어제(26일, 24~26도)보다 낮겠고, 아침 기온은 6~17도로 선선하겠습니다.<br />          특히, 내륙을 중심으로 낮과 밤의 기온차가 10도 내외로 크겠습니다.<br />○ (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다.\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "\n",
    "values = {\n",
    "    'stnId':'109'\n",
    "}\n",
    "\n",
    "params=parse.urlencode(values)\n",
    "url += \"?\"+params\n",
    "print(\"url=\", url)\n",
    "\n",
    "res = request.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "header = soup.find(\"header\")\n",
    "\n",
    "title = header.find(\"title\").text\n",
    "wf = header.find(\"wf\").text\n",
    "\n",
    "print(title)\n",
    "print(wf)\n",
    "\n",
    "title = soup.select_one(\"header > title\").text\n",
    "wf = header.select_one(\"header wf\").text\n",
    "\n",
    "print(title)\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. 윤동주 작가의 작품 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 하늘과 바람과 별과 시\n",
      "- 증보판\n",
      "- 서시\n",
      "- 자화상\n",
      "- 소년\n",
      "- 눈 오는 지도\n",
      "- 돌아와 보는 밤\n",
      "- 병원\n",
      "- 새로운 길\n",
      "- 간판 없는 거리\n",
      "- 태초의 아침\n",
      "- 또 태초의 아침\n",
      "- 새벽이 올 때까지\n",
      "- 무서운 시간\n",
      "- 십자가\n",
      "- 바람이 불어\n",
      "- 슬픈 족속\n",
      "- 눈감고 간다\n",
      "- 또 다른 고향\n",
      "- 길\n",
      "- 별 헤는 밤\n",
      "- 흰 그림자\n",
      "- 사랑스런 추억\n",
      "- 흐르는 거리\n",
      "- 쉽게 씌어진 시\n",
      "- 봄\n",
      "- 참회록\n",
      "- 간(肝)\n",
      "- 위로\n",
      "- 팔복\n",
      "- 못자는밤\n",
      "- 달같이\n",
      "- 고추밭\n",
      "- 아우의 인상화\n",
      "- 사랑의 전당\n",
      "- 이적\n",
      "- 비오는 밤\n",
      "- 산골물\n",
      "- 유언\n",
      "- 창\n",
      "- 바다\n",
      "- 비로봉\n",
      "- 산협의 오후\n",
      "- 명상\n",
      "- 소낙비\n",
      "- 한난계\n",
      "- 풍경\n",
      "- 달밤\n",
      "- 장\n",
      "- 밤\n",
      "- 황혼이 바다가 되어\n",
      "- 아침\n",
      "- 빨래\n",
      "- 꿈은 깨어지고\n",
      "- 산림\n",
      "- 이런날\n",
      "- 산상\n",
      "- 양지쪽\n",
      "- 닭\n",
      "- 가슴 1\n",
      "- 가슴 2\n",
      "- 비둘기\n",
      "- 황혼\n",
      "- 남쪽 하늘\n",
      "- 창공\n",
      "- 거리에서\n",
      "- 삶과 죽음\n",
      "- 초한대\n",
      "- 산울림\n",
      "- 해바라기 얼굴\n",
      "- 귀뚜라미와 나와\n",
      "- 애기의 새벽\n",
      "- 햇빛·바람\n",
      "- 반디불\n",
      "- 둘 다\n",
      "- 거짓부리\n",
      "- 눈\n",
      "- 참새\n",
      "- 버선본\n",
      "- 편지\n",
      "- 봄\n",
      "- 무얼 먹구 사나\n",
      "- 굴뚝\n",
      "- 햇비\n",
      "- 빗자루\n",
      "- 기왓장 내외\n",
      "- 오줌싸개 지도\n",
      "- 병아리\n",
      "- 조개껍질\n",
      "- 겨울\n",
      "- 트루게네프의 언덕\n",
      "- 달을 쏘다\n",
      "- 별똥 떨어진 데\n",
      "- 화원에 꽃이 핀다\n",
      "- 종시\n"
     ]
    }
   ],
   "source": [
    "url = \"https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\"\n",
    "res = request.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "a_list = soup.select(\"#mw-content-text   ul > li  a\")\n",
    "for a in a_list:\n",
    "    name = a.string\n",
    "    print(f\"- {name}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일반문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 다음 뉴스 헤드라인\n",
    "- 배운 내용을 바탕으로 다음 뉴스에서 헤드라인 뉴스의 제목을 추출해보고자 합니다\n",
    "    - Q. 다음의 코드에 CSS SELECTOR를 추가하여 최신 기사으 헤드라인을 스크레이핑하는 코드를 완성하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'대면 예배 금지 위법' 방역당국 고발한 교회단체..다른 단체는 \"대신 사과\"\n",
      "한국일보\n",
      "\n",
      "\n",
      "\"괜한 가족과 대판 싸움\"..사라진 일상에 '코로나 앵그리' 발톱\n",
      "중앙일보\n",
      "\n",
      "\n",
      "秋아들 \"휴가 승인 받아\".. 부대 관계자들 \"기억 없다\"\n",
      "국민일보\n",
      "\n",
      "\n",
      "최대집 의협 회장 및 임원, 탄핵 위기 모면..불신임안 '부결'\n",
      "연합뉴스\n",
      "\n",
      "\n",
      "정부 \"개헌절 집회차량 면허취소\"에 법조계 \"사실상 허가제, 위헌적 발상\"\n",
      "조선일보\n",
      "\n",
      "\n",
      "임대차법 탓에..\"석달만 사세요\" 단기임대 성황\n",
      "매일경제\n",
      "\n",
      "\n",
      "아무리 급했어도..보건소 절반 '미인증' 체온계 쓰고 있다\n",
      "머니투데이\n",
      "\n",
      "\n",
      "마크롱 \"벨라루스 독재권력 위기 맞았다..대통령 물러나야\"\n",
      "연합뉴스\n",
      "\n",
      "\n",
      "입지는 하남 교산, 당첨확률은 남양주 왕숙..30대에 '기회의 땅' 되나\n",
      "매일경제\n",
      "\n",
      "\n",
      "北 한 마디 할때마다 '들썩'..\"여야, 국내-국외정치 구분해야\"\n",
      "뉴스1\n",
      "\n",
      "\n",
      "美·中 반도체 전쟁 격화..삼성·하이닉스 '반사이익' Vs '고래싸움 새우등'\n",
      "이데일리\n",
      "\n",
      "\n",
      "서울여대 교수채용·재정 비리 의혹에 내홍..대학 \"허위\"\n",
      "한국일보\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://news.daum.net/\"\n",
    "\n",
    "res = request.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\") #BeautifulSoup 라이브러리를 사용하여 html에서 정보 추출\n",
    "\n",
    "selector = \"#mArticle > div.box_headline > ul.list_headline > li > strong\"\n",
    "\n",
    "for a in soup.select(selector):\n",
    "    title = a.text\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 시민의 소리 게시판\n",
    "- Q.다음의 코드에 css selector를 추가하여 해당 페이지에서 게시글의 제목을 스크레이핑하는 코드를 완성하시오. 또한 과제 제출시 하단의 추가 내용을 참고하여 수집한 데이터를 csv 형태로 저장하여 해당 csv 파일도 함께 제출하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['관리인 마스크', '어린이 대공원 쓰레기집하장 내 쓰레기 제거 요청 ', '마스크미착용으로 축구 및, 베트민턴 치는 인원이 너무 많아요.', '공원 내 마스크 착용', '청춘핫도그 점장님과 직원분께 감사드립니다', '카드결제를 거부하는 매점을 신고합니다', '참얼굴만큼예쁘고맘씨좋은 여직원을 만나 고마워서 글을남깁니다.', '놀이동산에서 불쾌함을 겪었습니다', '서문 플래카드', '간만에 친절한 아가씨를 만났어요.(놀이동산)'] ['https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=4HFFtmMKWw56DHfTHS5QYhQuBSh3f14ao1hqjDnwSUCaOZCTVIWyWnpQjgWh7RM9.etisw1_servlet_user?qnaid=QNAS20200917000010&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=4HFFtmMKWw56DHfTHS5QYhQuBSh3f14ao1hqjDnwSUCaOZCTVIWyWnpQjgWh7RM9.etisw1_servlet_user?qnaid=QNAS20200902000003&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=4HFFtmMKWw56DHfTHS5QYhQuBSh3f14ao1hqjDnwSUCaOZCTVIWyWnpQjgWh7RM9.etisw1_servlet_user?qnaid=QNAS20200826000002&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=4HFFtmMKWw56DHfTHS5QYhQuBSh3f14ao1hqjDnwSUCaOZCTVIWyWnpQjgWh7RM9.etisw1_servlet_user?qnaid=QNAS20200825000003&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=4HFFtmMKWw56DHfTHS5QYhQuBSh3f14ao1hqjDnwSUCaOZCTVIWyWnpQjgWh7RM9.etisw1_servlet_user?qnaid=QNAS20200818000009&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=4HFFtmMKWw56DHfTHS5QYhQuBSh3f14ao1hqjDnwSUCaOZCTVIWyWnpQjgWh7RM9.etisw1_servlet_user?qnaid=QNAS20200816000002&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=4HFFtmMKWw56DHfTHS5QYhQuBSh3f14ao1hqjDnwSUCaOZCTVIWyWnpQjgWh7RM9.etisw1_servlet_user?qnaid=QNAS20200813000003&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=4HFFtmMKWw56DHfTHS5QYhQuBSh3f14ao1hqjDnwSUCaOZCTVIWyWnpQjgWh7RM9.etisw1_servlet_user?qnaid=QNAS20200813000002&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=4HFFtmMKWw56DHfTHS5QYhQuBSh3f14ao1hqjDnwSUCaOZCTVIWyWnpQjgWh7RM9.etisw1_servlet_user?qnaid=QNAS20200730000004&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=4HFFtmMKWw56DHfTHS5QYhQuBSh3f14ao1hqjDnwSUCaOZCTVIWyWnpQjgWh7RM9.etisw1_servlet_user?qnaid=QNAS20200728000002&pgno=1']\n"
     ]
    }
   ],
   "source": [
    "url_head = \"https://www.sisul.or.kr\"\n",
    "\n",
    "url_board = url_head + \"/open_content/childrenpark/qna/qnaMsgList.do?pgno=1\"\n",
    "\n",
    "\n",
    "\n",
    "res = request.urlopen(url_board)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# selector = \"#detail_con > div.generalboard > table > tbody > tr > td.left.title > a\"\n",
    "selector = \"#detail_con > div.generalboard > table > tbody > tr > td.left.title > a\"\n",
    "titles = []\n",
    "links = []\n",
    "for a in soup.select(selector):\n",
    "    titles.append(a.text)\n",
    "    links.append(url_head + a.attrs[\"href\"])\n",
    "    \n",
    "print(titles, links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추가내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>관리인 마스크</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>어린이 대공원 쓰레기집하장 내 쓰레기 제거 요청</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>마스크미착용으로 축구 및, 베트민턴 치는 인원이 너무 많아요.</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>공원 내 마스크 착용</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>청춘핫도그 점장님과 직원분께 감사드립니다</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title  \\\n",
       "0                             관리인 마스크   \n",
       "1         어린이 대공원 쓰레기집하장 내 쓰레기 제거 요청    \n",
       "2  마스크미착용으로 축구 및, 베트민턴 치는 인원이 너무 많아요.   \n",
       "3                         공원 내 마스크 착용   \n",
       "4              청춘핫도그 점장님과 직원분께 감사드립니다   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.sisul.or.kr/open_content/childrenp...  \n",
       "1  https://www.sisul.or.kr/open_content/childrenp...  \n",
       "2  https://www.sisul.or.kr/open_content/childrenp...  \n",
       "3  https://www.sisul.or.kr/open_content/childrenp...  \n",
       "4  https://www.sisul.or.kr/open_content/childrenp...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "board_df = pd.DataFrame({\"title\": titles, \"link\": links})\n",
    "board_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_df.to_csv(\"board.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
